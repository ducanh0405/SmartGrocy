"""
WORKSTREAM 1 (OLIST) - PIPELINE T·ªîNG H·ª¢P

M·ª•c ƒë√≠ch:
1.  T·∫£i (Load) c√°c t·ªáp .csv c·ªßa Olist.
2.  H·ª£p nh·∫•t (Merge) ch√∫ng m·ªôt c√°ch an to√†n (x·ª≠ l√Ω b·∫´y 'payments').
3.  T·∫°o (Create) c√°c ƒë·∫∑c tr∆∞ng nghi·ªáp v·ª• (features), BAO G·ªíM geolocation & distance.
4.  S·ª≠a l·ªói (Fix) R√≤ r·ªâ D·ªØ li·ªáu (Data Leakage) trong ƒë·∫∑c tr∆∞ng review trung b√¨nh.
5.  L√†m s·∫°ch (Clean) & ƒêi·ªÅn Nulls (Impute) M·ªòT L·∫¶N ·ªü cu·ªëi.
6.  Ki·ªÉm tra (Validate) t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu cu·ªëi c√πng.
7.  Xu·∫•t (Save) ra m·ªôt file CSV cu·ªëi c√πng ƒë√£ l√†m s·∫°ch v√† t·ªëi ∆∞u.

C√°ch ch·∫°y (t·ª´ Terminal):
> pip install pandas numpy haversine pyarrow
> python completed_pipeline.py
"""

import pandas as pd
import numpy as np
import os
import sys
import time
from haversine import haversine # C·∫ßn c√†i ƒë·∫∑t: pip install haversine

# --- 1. C√ÅC H√ÄM T·∫¢I D·ªÆ LI·ªÜU ---

def load_data(data_dir='data/'):
    """T·∫£i t·∫•t c·∫£ c√°c t·ªáp CSV c·∫ßn thi·∫øt."""
    print(f"[B∆∞·ªõc 1/8] ƒêang t·∫£i d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c: {data_dir}...")
    files_to_keys = {
        'olist_orders_dataset.csv': 'orders', 'olist_order_items_dataset.csv': 'items',
        'olist_products_dataset.csv': 'products', 'olist_customers_dataset.csv': 'customers',
        'olist_order_reviews_dataset.csv': 'reviews', 'olist_order_payments_dataset.csv': 'payments',
        'olist_sellers_dataset.csv': 'sellers', 'olist_geolocation_dataset.csv': 'geolocation'
    }
    dataframes = {}
    try:
        for file, key in files_to_keys.items():
            file_path = os.path.join(data_dir, file)
            dataframes[key] = pd.read_csv(file_path)
        print(f"-> T·∫£i {len(dataframes)} t·ªáp d·ªØ li·ªáu ch√≠nh th√†nh c√¥ng.")
        print(f"-> C√°c kh√≥a (keys) ƒë√£ t·∫°o: {list(dataframes.keys())}")
        return dataframes
    except FileNotFoundError as e:
        print(f"üö® L·ªñI: Kh√¥ng t√¨m th·∫•y file {e.filename}. ƒê·∫£m b·∫£o c√°c t·ªáp CSV n·∫±m trong th∆∞ m·ª•c '{data_dir}'.")
        sys.exit(1)

def aggregate_payments(df_payments):
    """(QUAN TR·ªåNG) X·ª≠ l√Ω "B·∫´y H·ª£p nh·∫•t" üí£. G·ªôp b·∫£ng payments."""
    print("[B∆∞·ªõc 2/8] ƒêang g·ªôp (Aggregate) b·∫£ng 'payments'...")
    df_payments_agg = df_payments.groupby('order_id').agg(
        payment_installments_total=('payment_installments', 'sum'),
        payment_value_total=('payment_value', 'sum'),
        payment_type_primary=('payment_type', 'first')
    ).reset_index()
    print(f"-> ƒê√£ g·ªôp 'payments' t·ª´ {len(df_payments)} h√†ng xu·ªëng {len(df_payments_agg)} h√†ng.")
    return df_payments_agg

# --- 2. H√ÄM H·ª¢P NH·∫§T ---

def merge_tables(dataframes, df_payments_agg):
    """Th·ª±c thi pipeline h·ª£p nh·∫•t (merge) c√°c b·∫£ng."""
    print("[B∆∞·ªõc 3/8] ƒêang h·ª£p nh·∫•t (Merge) c√°c b·∫£ng...")
    df_master = dataframes['orders'].copy()
    df_master = pd.merge(df_master, dataframes['customers'], on='customer_id', how='left')
    df_reviews_dedup = dataframes['reviews'].drop_duplicates(subset='order_id', keep='last')
    df_master = pd.merge(df_master, df_reviews_dedup, on='order_id', how='left')
    df_master = pd.merge(df_master, df_payments_agg, on='order_id', how='left')
    df_master = pd.merge(df_master, dataframes['items'], on='order_id', how='left')
    df_master = pd.merge(df_master, dataframes['products'], on='product_id', how='left')
    df_master = pd.merge(df_master, dataframes['sellers'], on='seller_id', how='left')
    print(f"-> H·ª£p nh·∫•t (Merge) b·∫£ng l√µi th√†nh c√¥ng. K√≠ch th∆∞·ªõc b·∫£ng t·ªïng th·ªÉ: {df_master.shape}")
    return df_master

# --- 3. C√ÅC H√ÄM T·∫†O ƒê·∫∂C TR∆ØNG (CH∆ØA CLEAN) ---

def create_core_features(df_merged):
    """T·∫°o c√°c ƒë·∫∑c tr∆∞ng c∆° b·∫£n (th·ªùi gian, v·∫≠n h√†nh, thanh to√°n...)."""
    print("[B∆∞·ªõc 4/8] ƒêang t·∫°o c√°c ƒë·∫∑c tr∆∞ng c∆° b·∫£n (Core Features)...")
    df_featured = df_merged.copy()

    # Chuy·ªÉn ƒë·ªïi Th·ªùi gian
    time_cols = ['order_purchase_timestamp', 'order_approved_at',
                 'order_delivered_customer_date', 'order_estimated_delivery_date']
    for col in time_cols:
        df_featured[col] = pd.to_datetime(df_featured[col], errors='coerce')

    # ƒê·∫∑c tr∆∞ng V·∫≠n h√†nh (s·∫Ω c√≤n null n·∫øu ch∆∞a giao)
    df_featured['delivery_time_days'] = (df_featured['order_delivered_customer_date'] - df_featured['order_purchase_timestamp']).dt.total_seconds() / (24 * 60 * 60)
    df_featured['delivery_vs_estimated_days'] = (df_featured['order_estimated_delivery_date'] - df_featured['order_delivered_customer_date']).dt.total_seconds() / (24 * 60 * 60)

    # ƒê·∫∑c tr∆∞ng V·∫≠n chuy·ªÉn
    df_featured['freight_ratio'] = df_featured['freight_value'] / (df_featured['price'] + 1e-6)

    # ƒê·∫∑c tr∆∞ng Thanh to√°n
    df_featured['is_payment_credit_card'] = (df_featured['payment_type_primary'] == 'credit_card').astype(float) # D√πng float ƒë·ªÉ ch·ª©a NaN n·∫øu payment_type_primary l√† NaN
    df_featured['is_payment_boleto'] = (df_featured['payment_type_primary'] == 'boleto').astype(float)
    df_featured['is_payment_voucher'] = (df_featured['payment_type_primary'] == 'voucher').astype(float)
    df_featured['is_payment_installments'] = (df_featured['payment_installments_total'] > 1).astype(float) # D√πng float ƒë·ªÉ ch·ª©a NaN

    print("-> T·∫°o xong core features.")
    return df_featured

def add_geolocation_features(df_featured, df_geo):
    """(T·ªêI ∆ØU 2) T√≠ch h·ª£p geolocation v√† t√≠nh kho·∫£ng c√°ch."""
    print("[B∆∞·ªõc 5/8] ƒêang th√™m ƒë·∫∑c tr∆∞ng Geolocation (T·ªëi ∆∞u 2)...")
    df_geo_enriched = df_featured.copy()

    # Aggregate Geolocation
    df_geo_agg = df_geo.groupby('geolocation_zip_code_prefix').agg(
        geo_lat=('geolocation_lat', 'mean'),
        geo_lng=('geolocation_lng', 'mean')
    ).reset_index()

    # Merge l·∫ßn 1: Cho Customer
    df_geo_enriched = pd.merge(df_geo_enriched, df_geo_agg, left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left')
    df_geo_enriched.rename(columns={'geo_lat': 'customer_lat', 'geo_lng': 'customer_lng'}, inplace=True)
    df_geo_enriched.drop(columns=['geolocation_zip_code_prefix'], inplace=True, errors='ignore')

    # Merge l·∫ßn 2: Cho Seller
    df_geo_enriched = pd.merge(df_geo_enriched, df_geo_agg, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left', suffixes=('', '_seller_geo')) # Th√™m suffix ƒë·ªÉ tr√°nh tr√πng l·∫∑p
    df_geo_enriched.rename(columns={'geo_lat': 'seller_lat', 'geo_lng': 'seller_lng'}, inplace=True)
    df_geo_enriched.drop(columns=['geolocation_zip_code_prefix_seller_geo'], inplace=True, errors='ignore') # S·ª≠a t√™n c·ªôt drop

    # T√≠nh Kho·∫£ng c√°ch Haversine (km) - s·∫Ω c√≤n null n·∫øu thi·∫øu lat/lng
    locations_available = df_geo_enriched[['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng']].notnull().all(axis=1)
    distances = df_geo_enriched[locations_available].apply(
        lambda row: haversine((row['customer_lat'], row['customer_lng']), (row['seller_lat'], row['seller_lng'])),
        axis=1
    )
    df_geo_enriched['distance_seller_customer'] = np.nan
    df_geo_enriched.loc[locations_available, 'distance_seller_customer'] = distances

    print("-> ƒê√£ th√™m ƒë·∫∑c tr∆∞ng geolocation v√† distance (c√≥ th·ªÉ c√≤n nulls).")
    return df_geo_enriched

def fix_review_leakage(df_geo_enriched):
    """(T·ªêI ∆ØU 3 - V3) S·ª≠a l·ªói r√≤ r·ªâ d·ªØ li·ªáu review v√† x·ª≠ l√Ω nulls."""
    print("[B∆∞·ªõc 6/8] ƒêang t·∫°o ƒë·∫∑c tr∆∞ng review 'time-safe' (T·ªëi ∆∞u 3)...")
    df_reviews_fixed = df_geo_enriched.sort_values('order_purchase_timestamp').copy()

    # Pre-impute review_score g·ªëc
    mean_global_review = df_reviews_fixed['review_score'].mean()
    review_score_imputed = df_reviews_fixed['review_score'].fillna(mean_global_review)
    # print(f"-> ƒê√£ pre-impute c√°c Nulls trong review_score g·ªëc b·∫±ng global mean ({mean_global_review:.2f}).") # C√≥ th·ªÉ b·ªè print n√†y

    # T√≠nh expanding mean & shift TR√äN C·ªòT ƒê√É IMPUTE
    df_reviews_fixed['avg_review_score_product_ts'] = df_reviews_fixed.groupby('product_id')[review_score_imputed.name].expanding().mean().shift(1).reset_index(level=0, drop=True)
    df_reviews_fixed['avg_review_score_seller_ts'] = df_reviews_fixed.groupby('seller_id')[review_score_imputed.name].expanding().mean().shift(1).reset_index(level=0, drop=True)

    # Post-impute (nulls do shift(1))
    df_reviews_fixed['avg_review_score_product_ts'] = df_reviews_fixed['avg_review_score_product_ts'].fillna(mean_global_review)
    df_reviews_fixed['avg_review_score_seller_ts'] = df_reviews_fixed['avg_review_score_seller_ts'].fillna(mean_global_review)

    print("-> ƒê√£ t·∫°o ƒë·∫∑c tr∆∞ng review 'time-safe'.")
    return df_reviews_fixed

# --- 4. H√ÄM L√ÄM S·∫†CH & ƒêI·ªÄN NULLS CU·ªêI C√ôNG ---

def final_cleaning_and_imputation(df_featured_all):
    """L√†m s·∫°ch v√† ƒëi·ªÅn T·∫§T C·∫¢ nulls c√≤n l·∫°i."""
    print("[B∆∞·ªõc 7/8] ƒêang th·ª±c hi·ªán l√†m s·∫°ch cu·ªëi c√πng v√† ƒëi·ªÅn Nulls...")
    df_clean = df_featured_all.copy()

    # 1. L√†m s·∫°ch Cardinality
    df_clean['product_category_name'] = df_clean['product_category_name'].fillna('unknown').str.lower().str.strip()

    # 2. X·ª≠ l√Ω Outliers
    negative_delivery_mask = (df_clean['delivery_time_days'] < 0) & (df_clean['delivery_time_days'] != -999) # V·∫´n gi·ªØ l·∫°i -999 n·∫øu c√≥
    df_clean.loc[negative_delivery_mask, 'delivery_time_days'] = 0
    df_clean['freight_ratio'] = df_clean['freight_ratio'].clip(upper=10)

    # 3. ƒêi·ªÅn Nulls c√≤n l·∫°i (Imputation Chi·∫øn l∆∞·ª£c)
    # 3.1 Review Score g·ªëc (n·∫øu c√≤n s√≥t - kh√¥ng n√™n)
    df_clean['review_score'] = df_clean['review_score'].fillna(0)
    # 3.2 Delivery Times (ƒë∆°n ch∆∞a giao)
    df_clean['delivery_time_days'] = df_clean['delivery_time_days'].fillna(-999)
    df_clean['delivery_vs_estimated_days'] = df_clean['delivery_vs_estimated_days'].fillna(-999)
    # 3.3 Payment features
    payment_flags = ['is_payment_credit_card', 'is_payment_boleto', 'is_payment_voucher', 'is_payment_installments']
    for col in payment_flags:
        if col in df_clean.columns: df_clean[col] = df_clean[col].fillna(0).astype(int) # Chuy·ªÉn v·ªÅ int sau khi fillna
    df_clean['payment_installments_total'] = df_clean['payment_installments_total'].fillna(0)
    df_clean['payment_value_total'] = df_clean['payment_value_total'].fillna(0)
    # 3.4 Price/Freight
    df_clean['price'] = df_clean['price'].fillna(0)
    df_clean['freight_value'] = df_clean['freight_value'].fillna(0)
    df_clean['freight_ratio'] = df_clean['freight_ratio'].fillna(0) # N·∫øu price=0 v√† freight=0
    # 3.5 Geolocation features (ƒêi·ªÅn 0 n·∫øu thi·∫øu)
    geo_cols = ['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng', 'distance_seller_customer']
    for col in geo_cols:
        if col in df_clean.columns: df_clean[col] = df_clean[col].fillna(0)

    # 4. L√†m s·∫°ch cu·ªëi c√πng (lo·∫°i b·ªè h√†ng thi·∫øu kh√≥a ch√≠nh)
    df_clean.dropna(subset=['order_id', 'order_item_id'], inplace=True) # order_item_id c√≥ th·ªÉ v·∫´n null n·∫øu merge sai

    print("-> L√†m s·∫°ch cu·ªëi c√πng v√† ƒëi·ªÅn Nulls ho√†n t·∫•t.")
    return df_clean

# --- 5. H√ÄM KI·ªÇM TRA (VALIDATION FUNCTION) ---
# (Gi·ªØ nguy√™n h√†m validate_pipeline t·ª´ phi√™n b·∫£n tr∆∞·ªõc - n√≥ ƒë√£ ƒë√∫ng)
def validate_pipeline(df_final):
    """Th·ª±c thi 4 b√†i ki·ªÉm tra s·ª©c kh·ªèe ü©∫."""
    print("[B∆∞·ªõc 8/8] ƒêang ki·ªÉm tra (Validate) pipeline cu·ªëi c√πng...")
    is_valid = True
    # 1. Nulls
    final_nulls = df_final.isnull().sum().sum()
    if final_nulls > 0:
        print(f"-> üö® KI·ªÇM TRA 1 TH·∫§T B·∫†I: V·∫´n c√≤n {final_nulls} gi√° tr·ªã Null.")
        is_valid = False
    else:
        print("-> ‚úÖ Ki·ªÉm tra 1 (Nulls): ƒê·∫°t.")
    # 2. Distribution (Delivery Time)
    delivered_mask = df_final['delivery_time_days'] != -999
    if delivered_mask.any():
        min_real = df_final[delivered_mask]['delivery_time_days'].min()
        if min_real < 0:
            print(f"-> üö® KI·ªÇM TRA 2 TH·∫§T B·∫†I: 'delivery_time_days' < 0 ({min_real}).")
            is_valid = False
        else:
            print(f"-> ‚úÖ Ki·ªÉm tra 2 (Distribution): ƒê·∫°t. Min delivery time l√† {min_real:.2f} (>= 0).")
    else:
         print("-> üü° Ki·ªÉm tra 2 (Distribution): B·ªè qua (Kh√¥ng c√≥ ƒë∆°n giao?).")
    # 3. Cardinality
    nunique_categories = df_final['product_category_name'].nunique()
    print(f"-> ‚ÑπÔ∏è Ki·ªÉm tra 3 (Cardinality): T√¨m th·∫•y {nunique_categories} danh m·ª•c.")
    # 4. Integrity Check üö®
    key_columns = ['order_id', 'order_item_id']
    if all(col in df_final.columns for col in key_columns):
        # Quan tr·ªçng: Ph·∫£i x·ª≠ l√Ω null trong kh√≥a tr∆∞·ªõc khi ki·ªÉm tra duplicated
        df_check = df_final.copy()
        df_check[key_columns[0]] = df_check[key_columns[0]].fillna('MISSING_ORDER')
        df_check[key_columns[1]] = df_check[key_columns[1]].fillna('MISSING_ITEM') # D√πng fillna kh√°c 0
        duplicate_rows = df_check.duplicated(subset=key_columns).sum()

        if duplicate_rows > 0:
            print(f"-> üö® KI·ªÇM TRA 4 TH·∫§T B·∫†I: Pipeline t·∫°o ra {duplicate_rows} h√†ng tr√πng l·∫∑p.")
            is_valid = False
        else:
            print("-> ‚úÖ Ki·ªÉm tra 4 (Integrity): ƒê·∫°t.")
    else:
        print(f"-> üö® KI·ªÇM TRA 4 TH·∫§T B·∫†I: Thi·∫øu c·ªôt kh√≥a {key_columns}.")
        is_valid = False
    return is_valid

# --- 6. H√ÄM CH√çNH (MAIN FUNCTION) ---

def main():
    """ƒêi·ªÅu ph·ªëi to√†n b·ªô pipeline."""
    start_time = time.time()
    DATA_DIR = 'data/'
    OUTPUT_FILE_CSV = 'olist_master_table_optimized.csv'

    # --- Ch·∫°y Pipeline ---
    dataframes = load_data(DATA_DIR)
    df_payments_agg = aggregate_payments(dataframes['payments'])
    df_merged = merge_tables(dataframes, df_payments_agg)
    df_featured_core = create_core_features(df_merged)
    df_featured_geo = add_geolocation_features(df_featured_core, dataframes['geolocation'])
    df_featured_reviews = fix_review_leakage(df_featured_geo)
    df_final = final_cleaning_and_imputation(df_featured_reviews) # B∆∞·ªõc l√†m s·∫°ch cu·ªëi c√πng

    # --- Ki·ªÉm tra & L∆∞u ---
    is_pipeline_healthy = validate_pipeline(df_final)
    if is_pipeline_healthy:
        print(f"\n[B∆∞·ªõc 9/9] ƒêang l∆∞u tr·ªØ file {OUTPUT_FILE_CSV}...")
        try:
            final_columns = [ # Danh s√°ch c·ªôt cu·ªëi c√πng (ƒë√£ c·∫≠p nh·∫≠t)
                'order_id', 'order_item_id', 'product_id', 'customer_id', 'seller_id',
                'order_purchase_timestamp',
                'delivery_time_days', 'delivery_vs_estimated_days',
                'price', 'freight_value', 'freight_ratio',
                'is_payment_credit_card', 'is_payment_boleto', 'is_payment_installments', 'payment_value_total',
                'review_score',
                'avg_review_score_product_ts', 'avg_review_score_seller_ts',
                'distance_seller_customer',
                'product_category_name', 'customer_state', 'seller_state',
                'customer_lat', 'customer_lng', 'seller_lat', 'seller_lng'
            ]
            final_columns_exist = [col for col in final_columns if col in df_final.columns]
            df_final_output = df_final[final_columns_exist]

            df_final_output.to_csv(OUTPUT_FILE_CSV, index=False)
            print(f"\n--- ü•≥ HO√ÄN TH√ÄNH WORKSTREAM 1 (OPTIMIZED V3) ---")
            print(f"Output ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_FILE_CSV}")
            print(f"K√≠ch th∆∞·ªõc cu·ªëi c√πng: {df_final_output.shape}")
        except Exception as e:
            print(f"\nüö® L·ªñI khi l∆∞u file CSV: {e}")
    else:
        print("\nüö® L·ªñI: Pipeline kh√¥ng v∆∞·ª£t qua ki·ªÉm tra. S·∫Ω kh√¥ng l∆∞u file. Vui l√≤ng ki·ªÉm tra l·∫°i.")

    end_time = time.time()
    print(f"\nT·ªïng th·ªùi gian ch·∫°y pipeline: {end_time - start_time:.2f} gi√¢y.")

# --- ƒêI·ªÇM B·∫ÆT ƒê·∫¶U CH·∫†Y SCRIPT ---
if __name__ == "__main__":
    # C·∫•u h√¨nh Pandas ƒë·ªÉ x·ª≠ l√Ω l·ªói CopyWarning t·ªët h∆°n (t√πy ch·ªçn)
    pd.options.mode.chained_assignment = None # T·∫Øt c·∫£nh b√°o (ch·ªâ d√πng n·∫øu b·∫°n hi·ªÉu r√µ code)
    main()