{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Exploratory Data Analysis (EDA)\n",
        "\n",
        "KhÃ¡m phÃ¡ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u FreshRetail dataset.\n",
        "\n",
        "## Má»¥c tiÃªu\n",
        "- Hiá»ƒu cáº¥u trÃºc dá»¯ liá»‡u\n",
        "- PhÃ¡t hiá»‡n patterns vÃ  anomalies\n",
        "- Kiá»ƒm tra data quality\n",
        "- Visualize distributions vÃ  trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added D:\\datastorm\\E-Grocery_Forecaster\\src to sys.path\n",
            "Data quality libraries not installed. Install with: pip install great-expectations evidently\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'great_expectations'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_project_path, get_dataset_config, setup_logging\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_01_load_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_quality\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataQualityMonitor\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m comprehensive_validation\n\u001b[0;32m     22\u001b[0m setup_project_path()\n",
            "File \u001b[1;32mD:\\datastorm\\E-Grocery_Forecaster\\src\\utils\\data_quality.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectation_configuration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationConfiguration\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations'"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.config import setup_project_path, get_dataset_config, setup_logging\n",
        "from src.pipelines._01_load_data import load_data\n",
        "from src.utils.data_quality import DataQualityMonitor\n",
        "from src.utils.validation import comprehensive_validation\n",
        "\n",
        "setup_project_path()\n",
        "setup_logging()\n",
        "\n",
        "# Import display for Jupyter notebooks\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    # Fallback if not in Jupyter\n",
        "    display = print\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data vÃ  Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "dataframes, config = get_dataset_config(), None\n",
        "dataframes, config = load_data()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Dataset: {config['name']}\")\n",
        "print(f\"Temporal Unit: {config['temporal_unit']}\")\n",
        "print(f\"Time Column: {config['time_column']}\")\n",
        "print(f\"Target Column: {config['target_column']}\")\n",
        "print(f\"\\nLoaded DataFrames: {len(dataframes)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n{name.upper()}:\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {list(df.columns)[:10]}...\")\n",
        "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
        "    if len(df) > 0:\n",
        "        print(f\"  Date range: {df[config['time_column']].min()} to {df[config['time_column']].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Quality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data quality for each dataframe\n",
        "quality_monitor = DataQualityMonitor()\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DATA QUALITY CHECK: {name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    validation_results = comprehensive_validation(df, verbose=True)\n",
        "    quality_score = validation_results.get('quality_score', 0)\n",
        "    \n",
        "    print(f\"\\nOverall Quality Score: {quality_score}/100\")\n",
        "    \n",
        "    if quality_score >= 90:\n",
        "        print(\"âœ… EXCELLENT\")\n",
        "    elif quality_score >= 75:\n",
        "        print(\"âš ï¸ GOOD\")\n",
        "    else:\n",
        "        print(\"âŒ POOR - Needs attention\")\n",
        "    \n",
        "    # Store results\n",
        "    quality_monitor.store_validation_results(name, validation_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sales Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get sales dataframe\n",
        "sales_df = dataframes.get('sales') or dataframes.get('freshretail_train')\n",
        "if sales_df is None:\n",
        "    print(\"Sales dataframe not found!\")\n",
        "else:\n",
        "    print(f\"Sales Data Shape: {sales_df.shape}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(sales_df.head())\n",
        "    \n",
        "    print(f\"\\nBasic Statistics:\")\n",
        "    display(sales_df.describe())\n",
        "    \n",
        "    print(f\"\\nData Types:\")\n",
        "    display(sales_df.dtypes)\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nMissing Values:\")\n",
        "    missing = sales_df.isnull().sum()\n",
        "    display(missing[missing > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time Series Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate sales by time period\n",
        "if sales_df is not None and config:\n",
        "    time_col = config['time_column']\n",
        "    target_col = config['target_column']\n",
        "    \n",
        "    # Aggregate\n",
        "    if config['temporal_unit'] == 'hour':\n",
        "        # For hourly data, aggregate to daily\n",
        "        sales_df[time_col] = pd.to_datetime(sales_df[time_col])\n",
        "        sales_df['date'] = sales_df[time_col].dt.date\n",
        "        ts_data = sales_df.groupby('date')[target_col].sum().reset_index()\n",
        "        ts_data['date'] = pd.to_datetime(ts_data['date'])\n",
        "    else:\n",
        "        # For weekly data\n",
        "        ts_data = sales_df.groupby(time_col)[target_col].sum().reset_index()\n",
        "        ts_data = ts_data.sort_values(time_col)\n",
        "    \n",
        "    # Plot\n",
        "    fig = px.line(ts_data, x=ts_data.columns[0], y=target_col, \n",
        "                  title='Sales Over Time',\n",
        "                  labels={ts_data.columns[0]: 'Time', target_col: 'Sales'})\n",
        "    fig.update_layout(height=500)\n",
        "    fig.show()\n",
        "    \n",
        "    # Distribution\n",
        "    fig2 = px.histogram(sales_df, x=target_col, \n",
        "                       title='Sales Distribution',\n",
        "                       nbins=50)\n",
        "    fig2.update_layout(height=400)\n",
        "    fig2.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
