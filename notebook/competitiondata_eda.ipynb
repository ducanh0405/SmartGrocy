{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaefc88c",
   "metadata": {},
   "source": [
    "# ðŸ† Competition Data EDA -\n",
    "\n",
    "Exploratory Data Analysis chi tiáº¿t cho FreshRetail competition dataset.\n",
    "\n",
    "## Má»¥c tiÃªu\n",
    "- Hiá»ƒu cáº¥u trÃºc vÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a competition data\n",
    "- PhÃ¢n tÃ­ch patterns vÃ  seasonality\n",
    "- Identify data quality issues\n",
    "- Prepare insights cho feature engineering\n",
    "\n",
    "## Dataset Info\n",
    "- **Name**: FreshRetailNet-50K\n",
    "- **Type**: Hourly sales data\n",
    "- **Competition**: Time-series forecasting challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4cf5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added D:\\datastorm\\E-Grocery_Forecaster\\src to sys.path\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.config import setup_project_path, get_dataset_config, setup_logging\n",
    "from src.pipelines._01_load_data import load_data\n",
    "from src.utils.validation import comprehensive_validation\n",
    "\n",
    "setup_project_path()\n",
    "setup_logging()\n",
    "\n",
    "# Import display for Jupyter notebooks\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    # Fallback if not in Jupyter\n",
    "    display = print\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b91a70",
   "metadata": {},
   "source": [
    "## 1. Load Competition Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8b5222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:31:42,490 - src.pipelines._01_load_data - INFO - ======================================================================\n",
      "2025-11-13 01:31:42,492 - src.pipelines._01_load_data - INFO - [PIPELINE STEP 1: LOAD DATA]\n",
      "2025-11-13 01:31:42,493 - src.pipelines._01_load_data - INFO - Active Dataset: FreshRetailNet-50K\n",
      "2025-11-13 01:31:42,493 - src.pipelines._01_load_data - INFO - Data Directory: D:\\datastorm\\E-Grocery_Forecaster\\data\\2_raw\n",
      "2025-11-13 01:31:42,494 - src.pipelines._01_load_data - INFO - ======================================================================\n",
      "2025-11-13 01:31:42,495 - src.pipelines._01_load_data - INFO - Loading FreshRetailNet-50K dataset...\n",
      "2025-11-13 01:31:42,495 - src.pipelines._01_load_data - WARNING -   File not found: sales_hourly.parquet/csv\n",
      "2025-11-13 01:31:42,496 - src.pipelines._01_load_data - INFO -   Loading freshretail_train.parquet...\n",
      "2025-11-13 01:31:45,969 - src.pipelines._01_load_data - INFO - âœ“ Converted 'dt' to 'hour_timestamp'\n",
      "2025-11-13 01:31:45,978 - src.pipelines._01_load_data - INFO - âœ“ Converted 'sale_amount' to 'sales_quantity'\n",
      "2025-11-13 01:31:45,979 - src.pipelines._01_load_data - WARNING -   File not found: stockout_labels.parquet/csv\n",
      "2025-11-13 01:31:45,979 - src.pipelines._01_load_data - WARNING - Stockout labels not found, 'is_stockout' will be 0.\n",
      "2025-11-13 01:31:45,988 - src.pipelines._01_load_data - WARNING -   File not found: weather_data.parquet/csv\n",
      "2025-11-13 01:31:45,990 - src.pipelines._01_load_data - WARNING -   File not found: product_info.parquet/csv\n",
      "2025-11-13 01:31:45,990 - src.pipelines._01_load_data - INFO - Cleaning raw data (handling errors and outliers)...\n",
      "2025-11-13 01:31:47,168 - src.pipelines._01_load_data - INFO - âœ“ Raw data cleaning complete.\n",
      "2025-11-13 01:31:47,169 - src.pipelines._01_load_data - INFO - âœ… Data loading complete. Loaded 3 dataframes: ['sales', 'weather', 'products']\n",
      "======================================================================\n",
      "FRESHRETAIL COMPETITION DATASET\n",
      "======================================================================\n",
      "Dataset: FreshRetailNet-50K\n",
      "Temporal Unit: hour\n",
      "Time Column: hour_timestamp\n",
      "Target Column: sales_quantity\n",
      "\n",
      "DataFrames loaded: 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21120\\3768050603.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mTarget Column: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_column'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m\\nDataFrames loaded: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Get main sales dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0msales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sales'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'freshretail_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msales_df\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Try to find any dataframe with sales data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\datastorm\\E-Grocery_Forecaster\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe truth value of a \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is ambiguous. \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataframes, config = load_data()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FRESHRETAIL COMPETITION DATASET\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {config['name']}\")\n",
    "print(f\"Temporal Unit: {config['temporal_unit']}\")\n",
    "print(f\"Time Column: {config['time_column']}\")\n",
    "print(f\"Target Column: {config['target_column']}\")\n",
    "print(f\"\\nDataFrames loaded: {len(dataframes)}\")\n",
    "\n",
    "# Get main sales dataframe\n",
    "sales_df = dataframes.get('sales') or dataframes.get('freshretail_train')\n",
    "if sales_df is None:\n",
    "    # Try to find any dataframe with sales data\n",
    "    for name, df in dataframes.items():\n",
    "        if 'sales' in name.lower() or 'train' in name.lower():\n",
    "            sales_df = df\n",
    "            break\n",
    "\n",
    "if sales_df is not None:\n",
    "    print(f\"\\nMain Sales DataFrame: {sales_df.shape}\")\n",
    "    print(f\"Columns: {list(sales_df.columns)}\")\n",
    "    display(sales_df.head())\n",
    "else:\n",
    "    print(\"âš ï¸ Sales dataframe not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430024a9",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview & Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c157e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sales_df is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nShape: {sales_df.shape}\")\n",
    "    print(f\"Memory usage: {sales_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Data types:\\n{sales_df.dtypes}\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(f\"\\nMissing Values:\")\n",
    "    missing = sales_df.isnull().sum()\n",
    "    missing_pct = (missing / len(sales_df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Unique values\n",
    "    print(f\"\\nUnique Values:\")\n",
    "    for col in sales_df.columns:\n",
    "        n_unique = sales_df[col].nunique()\n",
    "        print(f\"  {col}: {n_unique} unique values\")\n",
    "    \n",
    "    # Time range\n",
    "    time_col = config['time_column']\n",
    "    if time_col in sales_df.columns:\n",
    "        print(f\"\\nTime Range:\")\n",
    "        print(f\"  Min: {sales_df[time_col].min()}\")\n",
    "        print(f\"  Max: {sales_df[time_col].max()}\")\n",
    "        print(f\"  Span: {sales_df[time_col].max() - sales_df[time_col].min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156893f",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27402f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sales_df is not None:\n",
    "    target_col = config['target_column']\n",
    "    \n",
    "    if target_col in sales_df.columns:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TARGET VARIABLE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        target_data = sales_df[target_col]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(target_data.describe())\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  Mean: {target_data.mean():.2f}\")\n",
    "        print(f\"  Median: {target_data.median():.2f}\")\n",
    "        print(f\"  Std: {target_data.std():.2f}\")\n",
    "        print(f\"  Min: {target_data.min():.2f}\")\n",
    "        print(f\"  Max: {target_data.max():.2f}\")\n",
    "        print(f\"  Skewness: {target_data.skew():.2f}\")\n",
    "        print(f\"  Kurtosis: {target_data.kurtosis():.2f}\")\n",
    "        \n",
    "        # Zero values\n",
    "        zero_count = (target_data == 0).sum()\n",
    "        zero_pct = zero_count / len(target_data) * 100\n",
    "        print(f\"\\nZero Values: {zero_count} ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        # Visualize distribution\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "        except ImportError:\n",
    "            display = print\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Distribution', 'Log Distribution', 'Box Plot', 'Q-Q Plot'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # Histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=target_data, nbinsx=50, name='Distribution'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Log histogram\n",
    "        log_target = np.log1p(target_data[target_data > 0])\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=log_target, nbinsx=50, name='Log Distribution'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Box plot\n",
    "        fig.add_trace(\n",
    "            go.Box(y=target_data, name='Box Plot'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=800, showlegend=False, title_text=\"Target Variable Analysis\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"âš ï¸ Target column '{target_col}' not found in dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b8684c",
   "metadata": {},
   "source": [
    "## 4. Time Series Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sales_df is not None and config['temporal_unit'] == 'hour':\n",
    "    time_col = config['time_column']\n",
    "    target_col = config['target_column']\n",
    "    \n",
    "    # Convert to datetime if needed\n",
    "    if sales_df[time_col].dtype != 'datetime64[ns]':\n",
    "        try:\n",
    "            sales_df[time_col] = pd.to_datetime(sales_df[time_col])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Aggregate by day for visualization\n",
    "    sales_df['date'] = sales_df[time_col].dt.date\n",
    "    daily_sales = sales_df.groupby('date')[target_col].sum().reset_index()\n",
    "    daily_sales['date'] = pd.to_datetime(daily_sales['date'])\n",
    "    \n",
    "    # Plot time series\n",
    "    fig = px.line(daily_sales, x='date', y=target_col,\n",
    "                  title='Daily Sales Over Time',\n",
    "                  labels={'date': 'Date', target_col: 'Total Sales'})\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Extract time features\n",
    "    sales_df['hour'] = sales_df[time_col].dt.hour\n",
    "    sales_df['day_of_week'] = sales_df[time_col].dt.dayofweek\n",
    "    sales_df['day_of_month'] = sales_df[time_col].dt.day\n",
    "    \n",
    "    # Hourly pattern\n",
    "    hourly_pattern = sales_df.groupby('hour')[target_col].mean()\n",
    "    fig2 = px.bar(x=hourly_pattern.index, y=hourly_pattern.values,\n",
    "                  title='Average Sales by Hour of Day',\n",
    "                  labels={'x': 'Hour', 'y': 'Average Sales'})\n",
    "    fig2.update_layout(height=400)\n",
    "    fig2.show()\n",
    "    \n",
    "    # Day of week pattern\n",
    "    dow_pattern = sales_df.groupby('day_of_week')[target_col].mean()\n",
    "    dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    fig3 = px.bar(x=[dow_names[i] for i in dow_pattern.index], \n",
    "                  y=dow_pattern.values,\n",
    "                  title='Average Sales by Day of Week',\n",
    "                  labels={'x': 'Day of Week', 'y': 'Average Sales'})\n",
    "    fig3.update_layout(height=400)\n",
    "    fig3.show()\n",
    "    \n",
    "elif sales_df is not None and config['temporal_unit'] == 'week':\n",
    "    time_col = config['time_column']\n",
    "    target_col = config['target_column']\n",
    "    \n",
    "    # Weekly aggregation\n",
    "    weekly_sales = sales_df.groupby(time_col)[target_col].sum().reset_index()\n",
    "    weekly_sales = weekly_sales.sort_values(time_col)\n",
    "    \n",
    "    # Plot\n",
    "    fig = px.line(weekly_sales, x=time_col, y=target_col,\n",
    "                  title='Weekly Sales Over Time',\n",
    "                  labels={time_col: 'Week', target_col: 'Total Sales'})\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ee599",
   "metadata": {},
   "source": [
    "## 5. Product & Store Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sales_df is not None:\n",
    "    target_col = config['target_column']\n",
    "    \n",
    "    # Product analysis\n",
    "    if 'PRODUCT_ID' in sales_df.columns or 'product_id' in sales_df.columns:\n",
    "        product_col = 'PRODUCT_ID' if 'PRODUCT_ID' in sales_df.columns else 'product_id'\n",
    "        \n",
    "        product_sales = sales_df.groupby(product_col)[target_col].sum().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"PRODUCT ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total Products: {sales_df[product_col].nunique()}\")\n",
    "        print(f\"\\nTop 10 Products by Sales:\")\n",
    "        display(product_sales.head(10))\n",
    "        \n",
    "        # Visualize\n",
    "        fig = px.bar(x=product_sales.head(20).index.astype(str), \n",
    "                     y=product_sales.head(20).values,\n",
    "                     title='Top 20 Products by Total Sales',\n",
    "                     labels={'x': 'Product ID', 'y': 'Total Sales'})\n",
    "        fig.update_layout(height=500, xaxis_tickangle=-45)\n",
    "        fig.show()\n",
    "    \n",
    "    # Store analysis\n",
    "    if 'STORE_ID' in sales_df.columns or 'store_id' in sales_df.columns:\n",
    "        store_col = 'STORE_ID' if 'STORE_ID' in sales_df.columns else 'store_id'\n",
    "        \n",
    "        store_sales = sales_df.groupby(store_col)[target_col].sum().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STORE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total Stores: {sales_df[store_col].nunique()}\")\n",
    "        print(f\"\\nTop 10 Stores by Sales:\")\n",
    "        display(store_sales.head(10))\n",
    "        \n",
    "        # Visualize\n",
    "        fig = px.bar(x=store_sales.head(20).index.astype(str), \n",
    "                     y=store_sales.head(20).values,\n",
    "                     title='Top 20 Stores by Total Sales',\n",
    "                     labels={'x': 'Store ID', 'y': 'Total Sales'})\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "    \n",
    "    # Product-Store combinations\n",
    "    if ('PRODUCT_ID' in sales_df.columns or 'product_id' in sales_df.columns) and \\\n",
    "       ('STORE_ID' in sales_df.columns or 'store_id' in sales_df.columns):\n",
    "        combo_cols = [product_col, store_col]\n",
    "        combo_sales = sales_df.groupby(combo_cols)[target_col].sum().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nTotal Product-Store Combinations: {len(combo_sales)}\")\n",
    "        print(f\"Average Sales per Combination: {combo_sales.mean():.2f}\")\n",
    "        print(f\"Median Sales per Combination: {combo_sales.median():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363d225",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91746dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sales_df is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run comprehensive validation\n",
    "    validation_results = comprehensive_validation(sales_df, verbose=True)\n",
    "    \n",
    "    quality_score = validation_results.get('quality_score', 0)\n",
    "    print(f\"\\nOverall Quality Score: {quality_score}/100\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"âœ… EXCELLENT - Data quality is very good\")\n",
    "    elif quality_score >= 75:\n",
    "        print(\"âš ï¸ GOOD - Data quality is acceptable\")\n",
    "    else:\n",
    "        print(\"âŒ POOR - Data quality needs attention\")\n",
    "    \n",
    "    # Check for specific issues\n",
    "    issues = validation_results.get('issues', [])\n",
    "    if issues:\n",
    "        print(f\"\\nIssues Found ({len(issues)}):\")\n",
    "        for i, issue in enumerate(issues[:10], 1):\n",
    "            print(f\"  {i}. {issue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78574408",
   "metadata": {},
   "source": [
    "## 7. Key Insights & Recommendations\n",
    "\n",
    "Tá»•ng há»£p insights vÃ  recommendations cho feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "insights = []\n",
    "\n",
    "if sales_df is not None:\n",
    "    target_col = config['target_column']\n",
    "    \n",
    "    # Insight 1: Sparsity\n",
    "    if 'PRODUCT_ID' in sales_df.columns or 'product_id' in sales_df.columns:\n",
    "        product_col = 'PRODUCT_ID' if 'PRODUCT_ID' in sales_df.columns else 'product_id'\n",
    "        time_col = config['time_column']\n",
    "        \n",
    "        # Check sparsity\n",
    "        total_combinations = sales_df[product_col].nunique() * sales_df[time_col].nunique()\n",
    "        actual_combinations = len(sales_df)\n",
    "        sparsity = (1 - actual_combinations / total_combinations) * 100\n",
    "        \n",
    "        insights.append(f\"ðŸ“Š Data Sparsity: {sparsity:.1f}% missing combinations\")\n",
    "        insights.append(\"   â†’ Recommendation: Use zero-filling in WS0 aggregation\")\n",
    "    \n",
    "    # Insight 2: Seasonality\n",
    "    if config['temporal_unit'] == 'hour':\n",
    "        insights.append(\"ðŸ“… Hourly Patterns Detected\")\n",
    "        insights.append(\"   â†’ Recommendation: Create hour-of-day, day-of-week features (WS2)\")\n",
    "    elif config['temporal_unit'] == 'week':\n",
    "        insights.append(\"ðŸ“… Weekly Patterns Detected\")\n",
    "        insights.append(\"   â†’ Recommendation: Create week-of-year, month features (WS2)\")\n",
    "    \n",
    "    # Insight 3: Target distribution\n",
    "    if target_col in sales_df.columns:\n",
    "        zero_pct = (sales_df[target_col] == 0).sum() / len(sales_df) * 100\n",
    "        if zero_pct > 20:\n",
    "            insights.append(f\"âš ï¸ High Zero Rate: {zero_pct:.1f}% zeros\")\n",
    "            insights.append(\"   â†’ Recommendation: Consider zero-inflated models or separate zero/non-zero prediction\")\n",
    "        \n",
    "        skewness = sales_df[target_col].skew()\n",
    "        if abs(skewness) > 2:\n",
    "            insights.append(f\"ðŸ“ˆ High Skewness: {skewness:.2f}\")\n",
    "            insights.append(\"   â†’ Recommendation: Consider log transformation or robust scaling\")\n",
    "\n",
    "print(\"\\n\".join(insights))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Run feature engineering pipeline (02_Feature_Engineering_Guide.ipynb)\")\n",
    "print(\"2. Train baseline models (baseline_model.ipynb)\")\n",
    "print(\"3. Train quantile models (03_Model_Training.ipynb)\")\n",
    "print(\"4. Generate predictions (04_Prediction_Forecasting.ipynb)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
