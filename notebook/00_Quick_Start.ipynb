{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Quick Start Guide - E-Grocery Forecaster\n",
        "\n",
        "HÆ°á»›ng dáº«n nhanh Ä‘á»ƒ cháº¡y pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i.\n",
        "\n",
        "## Má»¥c tiÃªu\n",
        "- Load dá»¯ liá»‡u tá»« FreshRetail dataset\n",
        "- Cháº¡y toÃ n bá»™ pipeline (WS0 â†’ WS6)\n",
        "- Train model vÃ  Ä‘Ã¡nh giÃ¡\n",
        "- Táº¡o predictions\n",
        "\n",
        "## Dataset\n",
        "- **Default**: FreshRetailNet-50K (hourly data)\n",
        "- **Location**: `data/2_raw/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: D:\\datastorm\\E-Grocery_Forecaster\n",
            "Python path: D:\\datastorm\\E-Grocery_Forecaster\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path: {sys.path[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added D:\\datastorm\\E-Grocery_Forecaster\\src to sys.path\n",
            "Dataset: FreshRetailNet-50K\n",
            "Temporal unit: hour\n",
            "Time column: hour_timestamp\n",
            "2025-11-13 01:25:32,323 - src.pipelines._01_load_data - INFO - ======================================================================\n",
            "2025-11-13 01:25:32,323 - src.pipelines._01_load_data - INFO - [PIPELINE STEP 1: LOAD DATA]\n",
            "2025-11-13 01:25:32,324 - src.pipelines._01_load_data - INFO - Active Dataset: FreshRetailNet-50K\n",
            "2025-11-13 01:25:32,325 - src.pipelines._01_load_data - INFO - Data Directory: D:\\datastorm\\E-Grocery_Forecaster\\data\\2_raw\n",
            "2025-11-13 01:25:32,325 - src.pipelines._01_load_data - INFO - ======================================================================\n",
            "2025-11-13 01:25:32,326 - src.pipelines._01_load_data - INFO - Loading FreshRetailNet-50K dataset...\n",
            "2025-11-13 01:25:32,327 - src.pipelines._01_load_data - WARNING -   File not found: sales_hourly.parquet/csv\n",
            "2025-11-13 01:25:32,327 - src.pipelines._01_load_data - INFO -   Loading freshretail_train.parquet...\n",
            "2025-11-13 01:25:36,607 - src.pipelines._01_load_data - INFO - âœ“ Converted 'dt' to 'hour_timestamp'\n",
            "2025-11-13 01:25:36,625 - src.pipelines._01_load_data - INFO - âœ“ Converted 'sale_amount' to 'sales_quantity'\n",
            "2025-11-13 01:25:36,627 - src.pipelines._01_load_data - WARNING -   File not found: stockout_labels.parquet/csv\n",
            "2025-11-13 01:25:36,627 - src.pipelines._01_load_data - WARNING - Stockout labels not found, 'is_stockout' will be 0.\n",
            "2025-11-13 01:25:36,634 - src.pipelines._01_load_data - WARNING -   File not found: weather_data.parquet/csv\n",
            "2025-11-13 01:25:36,635 - src.pipelines._01_load_data - WARNING -   File not found: product_info.parquet/csv\n",
            "2025-11-13 01:25:36,636 - src.pipelines._01_load_data - INFO - Cleaning raw data (handling errors and outliers)...\n",
            "2025-11-13 01:25:37,803 - src.pipelines._01_load_data - INFO - âœ“ Raw data cleaning complete.\n",
            "2025-11-13 01:25:37,804 - src.pipelines._01_load_data - INFO - âœ… Data loading complete. Loaded 3 dataframes: ['sales', 'weather', 'products']\n",
            "\n",
            "Loaded 3 dataframes:\n",
            "  - sales: (4500000, 22)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataframes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataframes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m dataframes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "from src.config import setup_project_path, get_dataset_config, setup_logging\n",
        "from src.pipelines._01_load_data import load_data\n",
        "\n",
        "setup_project_path()\n",
        "setup_logging()\n",
        "\n",
        "# Get config\n",
        "config = get_dataset_config()\n",
        "print(f\"Dataset: {config['name']}\")\n",
        "print(f\"Temporal unit: {config['temporal_unit']}\")\n",
        "print(f\"Time column: {config['time_column']}\")\n",
        "\n",
        "# Load data\n",
        "dataframes, config = load_data()\n",
        "print(f\"\\nLoaded {len(dataframes)} dataframes:\")\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"  - {name}: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Feature Engineering (WS0-WS6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-13 01:26:06,102 - src.pipelines._02_feature_enrichment - INFO - ======================================================================\n",
            "2025-11-13 01:26:06,102 - src.pipelines._02_feature_enrichment - INFO - STARTING FEATURE ENRICHMENT PIPELINE\n",
            "2025-11-13 01:26:06,103 - src.pipelines._02_feature_enrichment - INFO - ======================================================================\n",
            "2025-11-13 01:26:06,103 - src.pipelines._01_load_data - INFO - ======================================================================\n",
            "2025-11-13 01:26:06,104 - src.pipelines._01_load_data - INFO - [PIPELINE STEP 1: LOAD DATA]\n",
            "2025-11-13 01:26:06,104 - src.pipelines._01_load_data - INFO - Active Dataset: FreshRetailNet-50K\n",
            "2025-11-13 01:26:06,104 - src.pipelines._01_load_data - INFO - Data Directory: D:\\datastorm\\E-Grocery_Forecaster\\data\\2_raw\n",
            "2025-11-13 01:26:06,104 - src.pipelines._01_load_data - INFO - ======================================================================\n",
            "2025-11-13 01:26:06,105 - src.pipelines._01_load_data - INFO - Loading FreshRetailNet-50K dataset...\n",
            "2025-11-13 01:26:06,105 - src.pipelines._01_load_data - WARNING -   File not found: sales_hourly.parquet/csv\n",
            "2025-11-13 01:26:06,105 - src.pipelines._01_load_data - INFO -   Loading freshretail_train.parquet...\n",
            "2025-11-13 01:26:09,800 - src.pipelines._01_load_data - INFO - âœ“ Converted 'dt' to 'hour_timestamp'\n",
            "2025-11-13 01:26:09,808 - src.pipelines._01_load_data - INFO - âœ“ Converted 'sale_amount' to 'sales_quantity'\n",
            "2025-11-13 01:26:09,809 - src.pipelines._01_load_data - WARNING -   File not found: stockout_labels.parquet/csv\n",
            "2025-11-13 01:26:09,809 - src.pipelines._01_load_data - WARNING - Stockout labels not found, 'is_stockout' will be 0.\n",
            "2025-11-13 01:26:09,815 - src.pipelines._01_load_data - WARNING -   File not found: weather_data.parquet/csv\n",
            "2025-11-13 01:26:09,816 - src.pipelines._01_load_data - WARNING -   File not found: product_info.parquet/csv\n",
            "2025-11-13 01:26:09,816 - src.pipelines._01_load_data - INFO - Cleaning raw data (handling errors and outliers)...\n",
            "2025-11-13 01:26:11,002 - src.pipelines._01_load_data - INFO - âœ“ Raw data cleaning complete.\n",
            "2025-11-13 01:26:11,002 - src.pipelines._01_load_data - INFO - âœ… Data loading complete. Loaded 3 dataframes: ['sales', 'weather', 'products']\n",
            "2025-11-13 01:26:11,003 - src.pipelines._02_feature_enrichment - INFO - Base dataframe 'sales' loaded. Shape: (4500000, 22)\n",
            "2025-11-13 01:26:11,003 - src.pipelines._02_feature_enrichment - INFO - --- (2/8) Workstream 0: Aggregation & Master Grid ---\n",
            "2025-11-13 01:26:11,004 - src.features.ws0_aggregation - INFO - WS0-Config: Creating master grid for hour-level data\n",
            "2025-11-13 01:26:11,004 - src.features.ws0_aggregation - INFO - WS0-Config: Groupby keys: ['product_id', 'store_id', 'hour_timestamp']\n",
            "2025-11-13 01:26:11,005 - src.features.ws0_aggregation - INFO - WS0-Config: Time column: hour_timestamp\n",
            "2025-11-13 01:26:11,005 - src.features.ws0_aggregation - INFO - WS0-Config: Has stockout: True\n",
            "2025-11-13 01:26:11,006 - src.features.ws0_aggregation - INFO - WS0-Config: Aggregation rules: {'sales_quantity': 'sum'}\n",
            "2025-11-13 01:26:12,503 - src.features.ws0_aggregation - INFO - WS0-Config: Aggregated 4,500,000 -> 4,500,000 hourly records\n",
            "2025-11-13 01:26:12,504 - src.features.ws0_aggregation - INFO - WS0-Config: Stockout dataset - keeping all records (including zeros)\n",
            "2025-11-13 01:26:12,555 - src.features.ws0_aggregation - INFO - WS0-Config: Grid dimensions: 2 entities Ã— 90 hours\n",
            "2025-11-13 01:28:38,936 - src.features.ws0_aggregation - INFO - WS0-Config: Grid complete: 69,909,300 rows (65,409,300 zero-filled)\n",
            "2025-11-13 01:28:43,462 - src.pipelines._02_feature_enrichment - INFO - âœ“ WS0 complete - Shape: (69909300, 4)\n",
            "2025-11-13 01:28:43,463 - src.pipelines._02_feature_enrichment - INFO - --- (3/8) Workstream 1: Relational Features ---\n",
            "2025-11-13 01:28:43,464 - src.features.ws1_relational_features - INFO - [WS1] Enriching relational features (Product, Household)...\n",
            "2025-11-13 01:28:43,464 - src.features.ws1_relational_features - WARNING - SKIPPING WS1: 'product' dataframe not found in dataframes_dict.\n",
            "2025-11-13 01:28:43,465 - src.pipelines._02_feature_enrichment - INFO - âœ“ WS1 complete - Shape: (69909300, 4)\n",
            "2025-11-13 01:28:43,465 - src.pipelines._02_feature_enrichment - INFO - --- (4/8) Workstream 5: Stockout Recovery ---\n",
            "2025-11-13 01:28:43,466 - src.pipelines._02_feature_enrichment - INFO - Running WS5 (Latent Demand) in parallel (12 jobs)...\n",
            "2025-11-13 01:29:14,495 - src.utils.parallel_processing - INFO - Parallel processing 776,770 groups with 12 jobs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=12)]: Using backend MultiprocessingBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    5.7s\n",
            "[Parallel(n_jobs=12)]: Done 1048 tasks      | elapsed:    6.0s\n",
            "[Parallel(n_jobs=12)]: Done 29976 tasks      | elapsed:   11.1s\n",
            "[Parallel(n_jobs=12)]: Done 64920 tasks      | elapsed:   18.6s\n",
            "[Parallel(n_jobs=12)]: Done 108120 tasks      | elapsed:   28.2s\n",
            "[Parallel(n_jobs=12)]: Done 169944 tasks      | elapsed:   37.3s\n",
            "[Parallel(n_jobs=12)]: Done 229656 tasks      | elapsed:   49.5s\n",
            "[Parallel(n_jobs=12)]: Done 309784 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=12)]: Done 390600 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=12)]: Done 474992 tasks      | elapsed:  1.8min\n"
          ]
        }
      ],
      "source": [
        "from src.pipelines._02_feature_enrichment import main as run_feature_enrichment\n",
        "\n",
        "# Run feature enrichment pipeline\n",
        "run_feature_enrichment()\n",
        "\n",
        "# Load master feature table\n",
        "from src.config import OUTPUT_FILES\n",
        "master_df = pd.read_parquet(OUTPUT_FILES['master_feature_table'])\n",
        "print(f\"Master feature table shape: {master_df.shape}\")\n",
        "print(f\"\\nColumns ({len(master_df.columns)}):\")\n",
        "print(master_df.columns.tolist()[:10], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "from src.pipelines._03_model_training import main as train_models\n",
        "\n",
        "# Create args for training\n",
        "class Args:\n",
        "    tune = False\n",
        "    full_data = False\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Train models\n",
        "train_models(args)\n",
        "\n",
        "print(\"\\nâœ… Model training complete!\")\n",
        "print(\"Check models/ directory for trained models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pipelines._05_prediction import main as make_predictions\n",
        "\n",
        "# Make predictions\n",
        "make_predictions()\n",
        "\n",
        "print(\"\\nâœ… Predictions complete!\")\n",
        "print(\"Check reports/ directory for prediction results\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
