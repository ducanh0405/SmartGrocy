{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Model Training & Evaluation\n",
        "\n",
        "H∆∞·ªõng d·∫´n training quantile regression models v√† ƒë√°nh gi√° k·∫øt qu·∫£.\n",
        "\n",
        "## M·ª•c ti√™u\n",
        "- Load feature table ƒë√£ ƒë∆∞·ª£c engineering\n",
        "- Split data theo time-based (leak-safe)\n",
        "- Train 7 quantile models (Q05, Q10, Q25, Q50, Q75, Q90, Q95)\n",
        "- ƒê√°nh gi√° v·ªõi Pinball Loss v√† Prediction Interval Coverage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: FreshRetailNet-50K\n",
            "Target: sales_quantity\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.config import setup_project_path, get_dataset_config, OUTPUT_FILES, setup_logging\n",
        "from src.pipelines._03_model_training import (\n",
        "    load_data, prepare_data, train_quantile_models, evaluate_quantile_models\n",
        ")\n",
        "\n",
        "setup_project_path()\n",
        "setup_logging()\n",
        "\n",
        "# Import display for Jupyter notebooks\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    # Fallback if not in Jupyter\n",
        "    display = print\n",
        "\n",
        "# Get config\n",
        "config = get_dataset_config()\n",
        "print(f\"Dataset: {config['name']}\")\n",
        "print(f\"Target: {config['target_column']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Feature Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-13 01:31:29,953 - src.pipelines._03_model_training - INFO - Loading data from: D:\\datastorm\\E-Grocery_Forecaster\\data\\3_processed\\master_feature_table.parquet...\n",
            "2025-11-13 01:31:29,954 - src.pipelines._03_model_training - ERROR - File not found: D:\\datastorm\\E-Grocery_Forecaster\\data\\3_processed\\master_feature_table.parquet. Run _02_feature_enrichment.py first.\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
          ]
        }
      ],
      "source": [
        "# Load master feature table\n",
        "df = load_data(OUTPUT_FILES['master_feature_table'])\n",
        "print(f\"Feature table shape: {df.shape}\")\n",
        "print(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "# Show basic info\n",
        "print(f\"\\nDate range:\")\n",
        "if config['temporal_unit'] == 'week':\n",
        "    print(f\"  Weeks: {df[config['time_column']].min()} to {df[config['time_column']].max()}\")\n",
        "else:\n",
        "    time_col = config['time_column']\n",
        "    if time_col in df.columns:\n",
        "        print(f\"  Range: {df[time_col].min()} to {df[time_col].max()}\")\n",
        "\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data & Time-Based Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data with time-based split\n",
        "X_train, X_test, y_train, y_test, features, cat_features = prepare_data(df, config)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nFeatures: {len(features)}\")\n",
        "print(f\"Categorical features: {len(cat_features)}\")\n",
        "\n",
        "# Verify leak-safety\n",
        "if config['temporal_unit'] == 'week':\n",
        "    train_weeks = df.loc[X_train.index, config['time_column']].max()\n",
        "    test_weeks = df.loc[X_test.index, config['time_column']].min()\n",
        "    print(f\"\\n‚úÖ Leak-safe check:\")\n",
        "    print(f\"  Train max week: {train_weeks}\")\n",
        "    print(f\"  Test min week: {test_weeks}\")\n",
        "    print(f\"  Gap: {test_weeks - train_weeks} weeks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Quantile Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all quantile models\n",
        "quantile_models = train_quantile_models(X_train, y_train, cat_features)\n",
        "\n",
        "print(f\"Trained {len(quantile_models)} quantile models:\")\n",
        "for q, model in quantile_models.items():\n",
        "    print(f\"  Q{int(q*100):02d}: {type(model).__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate models\n",
        "metrics = evaluate_quantile_models(quantile_models, X_test, y_test)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Pinball losses\n",
        "print(\"\\nPinball Losses:\")\n",
        "for q in sorted(quantile_models.keys()):\n",
        "    key = f\"q{int(q*100):02d}_pinball_loss\"\n",
        "    if key in metrics:\n",
        "        print(f\"  Q{int(q*100):02d}: {metrics[key]:.4f}\")\n",
        "\n",
        "# Coverage\n",
        "print(\"\\nPrediction Interval Coverage:\")\n",
        "for interval in ['90', '80']:\n",
        "    key = f\"prediction_interval_coverage_{interval}\"\n",
        "    if key in metrics:\n",
        "        print(f\"  {interval}% interval: {metrics[key]:.4f}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics_file = OUTPUT_FILES['reports_dir'] / 'metrics' / 'quantile_model_metrics.json'\n",
        "metrics_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(metrics_file, 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(f\"\\n‚úÖ Metrics saved to: {metrics_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
