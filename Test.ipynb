{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducanh0405/datastorm/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YWZZc83t1J2",
        "outputId": "9d235310-8006-40f9-97ab-9fa6577cb73d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# --- B∆Ø·ªöC 0: T·∫¢I V√Ä XEM L·∫†I D·ªÆ LI·ªÜU ---\n",
        "try:\n",
        "    # Load the advanced features dataset which includes the 'rating_category'\n",
        "    df = pd.read_csv('/content/processed_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\")\n",
        "\n",
        "    # Remove columns not needed for training\n",
        "    # ID and Delivery_person_ID are not features for learning\n",
        "    # Also remove original date/time columns as cyclical features are created\n",
        "    df_model = df.drop(columns=['ID', 'Delivery_person_ID', 'Order_Date', 'Time_Orderd', 'Time_Order_picked'])\n",
        "\n",
        "    # --- B∆Ø·ªöC 1: M√É H√ìA (ENCODING) D·ªÆ LI·ªÜU CH·ªÆ ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\")\n",
        "\n",
        "    # Get list of columns to encode, including the new 'rating_category'\n",
        "    categorical_cols = [\n",
        "        'Weatherconditions',\n",
        "        'Road_traffic_density',\n",
        "        'Type_of_order',\n",
        "        'Festival',\n",
        "        'City',\n",
        "        'rating_category', # Include the binned rating category\n",
        "        'traffic_time_interaction' # Include the new interaction feature\n",
        "    ]\n",
        "\n",
        "    # Perform One-Hot Encoding\n",
        "    # Check if columns exist before encoding\n",
        "    cols_to_encode_exist = [col for col in categorical_cols if col in df_model.columns]\n",
        "    df_encoded = pd.get_dummies(df_model, columns=cols_to_encode_exist, drop_first=True)\n",
        "\n",
        "    print(\"‚úÖ M√£ h√≥a th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë c·ªôt sau khi m√£ h√≥a: {df_encoded.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 2: PH√ÇN CHIA D·ªÆ LI·ªÜU (TRAIN-TEST SPLIT) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\")\n",
        "\n",
        "    # 'X' is all feature columns (input data)\n",
        "    X = df_encoded.drop('Time_taken (min)', axis=1)\n",
        "\n",
        "    # 'y' is the target column we want to predict\n",
        "    y = df_encoded['Time_taken (min)']\n",
        "\n",
        "    # Split data: 80% for training, 20% for testing\n",
        "    # random_state=42 to ensure the split is the same every time\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n chia th√†nh c√¥ng!\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): {X_train.shape[0]} d√≤ng\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): {X_test.shape[0]} d√≤ng\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 3: L·ª∞A CH·ªåN V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\")\n",
        "\n",
        "    # Initialize the model\n",
        "    # n_estimators=100: the model will create 100 \"decision trees\"\n",
        "    # n_jobs=-1: use all available CPUs for faster training\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # \"Teach\" the model using training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 4: ƒê√ÅNH GI√Å M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\")\n",
        "\n",
        "    # Ask the model to predict on the test set (data it hasn't seen before)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Measure error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f} ph√∫t\")\n",
        "    print(f\"Ch·ªâ s·ªë R-squared (R¬≤): {r2:.2f}\")\n",
        "    print(\"\\n--- Di·ªÖn gi·∫£i ---\")\n",
        "    print(f\"-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng {mae:.2f} ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\")\n",
        "    print(f\"-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng {r2:.0%} s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu! Vui l√≤ng ƒë·∫£m b·∫£o file 'advanced_features_dataset.csv' t·ªìn t·∫°i.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUQ6ImTRobd",
        "outputId": "933ee733-081d-4757-e10a-e890bde41ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\n",
            "‚úÖ M√£ h√≥a th√†nh c√¥ng!\n",
            "S·ªë c·ªôt sau khi m√£ h√≥a: 29\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\n",
            "‚úÖ Ph√¢n chia th√†nh c√¥ng!\n",
            "K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): 35089 d√≤ng\n",
            "K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): 8773 d√≤ng\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\n",
            "‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\n",
            "\n",
            "‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\n",
            "\n",
            "--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\n",
            "Mean Absolute Error (MAE): 4.70 ph√∫t\n",
            "Ch·ªâ s·ªë R-squared (R¬≤): 0.58\n",
            "\n",
            "--- Di·ªÖn gi·∫£i ---\n",
            "-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng 4.70 ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\n",
            "-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng 58% s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# --- B∆Ø·ªöC 0: T·∫¢I V√Ä XEM L·∫†I D·ªÆ LI·ªÜU ---\n",
        "try:\n",
        "    # Load the advanced features dataset which includes the 'rating_category'\n",
        "    df = pd.read_csv('/content/advanced_features_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'advanced_features_dataset.csv' th√†nh c√¥ng!\")\n",
        "\n",
        "    # Remove columns not needed for training\n",
        "    # ID and Delivery_person_ID are not features for learning\n",
        "    # Also remove original date/time columns as cyclical features are created\n",
        "    df_model = df.drop(columns=['ID', 'Delivery_person_ID', 'Order_Date', 'Time_Orderd', 'Time_Order_picked'])\n",
        "\n",
        "    # --- B∆Ø·ªöC 1: M√É H√ìA (ENCODING) D·ªÆ LI·ªÜU CH·ªÆ ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu m√£ h√≥a c√°c c·ªôt ph√¢n lo·∫°i...\")\n",
        "\n",
        "    # Get list of columns to encode, including the new 'rating_category'\n",
        "    categorical_cols = [\n",
        "        'Weatherconditions',\n",
        "        'Road_traffic_density',\n",
        "        'Type_of_order',\n",
        "        'Festival',\n",
        "        'City',\n",
        "        'rating_category', # Include the binned rating category\n",
        "        'traffic_time_interaction' # Include the new interaction feature\n",
        "    ]\n",
        "\n",
        "    # Perform One-Hot Encoding\n",
        "    # Check if columns exist before encoding\n",
        "    cols_to_encode_exist = [col for col in categorical_cols if col in df_model.columns]\n",
        "    df_encoded = pd.get_dummies(df_model, columns=cols_to_encode_exist, drop_first=True)\n",
        "\n",
        "    print(\"‚úÖ M√£ h√≥a th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë c·ªôt sau khi m√£ h√≥a: {df_encoded.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 2: PH√ÇN CHIA D·ªÆ LI·ªÜU (TRAIN-TEST SPLIT) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ph√¢n chia d·ªØ li·ªáu...\")\n",
        "\n",
        "    # 'X' is all feature columns (input data)\n",
        "    X = df_encoded.drop('Time_taken (min)', axis=1)\n",
        "\n",
        "    # 'y' is the target column we want to predict\n",
        "    y = df_encoded['Time_taken (min)']\n",
        "\n",
        "    # Split data: 80% for training, 20% for testing\n",
        "    # random_state=42 to ensure the split is the same every time\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n chia th√†nh c√¥ng!\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (train): {X_train.shape[0]} d√≤ng\")\n",
        "    print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra (test): {X_test.shape[0]} d√≤ng\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 3: L·ª∞A CH·ªåN V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...\")\n",
        "\n",
        "    # Initialize the model\n",
        "    # n_estimators=100: the model will create 100 \"decision trees\"\n",
        "    # n_jobs=-1: use all available CPUs for faster training\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # \"Teach\" the model using training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!\")\n",
        "\n",
        "    # --- B∆Ø·ªöC 4: ƒê√ÅNH GI√Å M√î H√åNH ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh...\")\n",
        "\n",
        "    # Ask the model to predict on the test set (data it hasn't seen before)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Measure error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ---\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f} ph√∫t\")\n",
        "    print(f\"Ch·ªâ s·ªë R-squared (R¬≤): {r2:.2f}\")\n",
        "    print(\"\\n--- Di·ªÖn gi·∫£i ---\")\n",
        "    print(f\"-> MAE: Trung b√¨nh, m√¥ h√¨nh d·ª± ƒëo√°n sai l·ªách kho·∫£ng {mae:.2f} ph√∫t so v·ªõi th·ªùi gian th·ª±c t·∫ø.\")\n",
        "    print(f\"-> R¬≤: M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c kho·∫£ng {r2:.0%} s·ª± bi·∫øn ƒë·ªông c·ªßa th·ªùi gian giao h√†ng.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu! Vui l√≤ng ƒë·∫£m b·∫£o file 'advanced_features_dataset.csv' t·ªìn t·∫°i.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-nLEN65PZ2-"
      },
      "source": [
        "x·ª≠ l√Ω ƒë·ªÉ c·∫£i thi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORjLVIIIPgEI",
        "outputId": "0d8bc728-5940-446c-ebc2-9cb5322cbe78"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- B∆Ø·ªö·ªöC 0: T·∫¢·∫¢I D·ªÆ LI·ªÜ·ªÜU ---\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- B∆Ø·ªö·ªöC 0: T·∫¢·∫¢I D·ªÆ LI·ªÜ·ªÜU ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/processed_dataset.csv')\n",
        "    print(\"‚úÖ ƒê·ªçc file 'processed_dataset.csv' th√†nh c√¥ng!\")\n",
        "    print(f\"S·ªë d√≤ng ban ƒë·∫ßu: {df.shape[0]}, S·ªë c·ªôt ban ƒë·∫ßu: {df.shape[1]}\")\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 1: PH√ÇN NH√ìM/R·ªúI R·∫†C H√ìA (BINNING) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu Ph√¢n nh√≥m c·ªôt 'Delivery_person_Ratings'...\")\n",
        "\n",
        "    # ƒê·ªãnh nghƒ©a c√°c kho·∫£ng gi√° tr·ªã (bins) v√† nh√£n (labels)\n",
        "    bins = [0, 4.5, 4.8, 5.1]  # Kho·∫£ng gi√° tr·ªã: (0, 4.5], (4.5, 4.8], (4.8, 5.1]\n",
        "    labels = ['Rating_Thap', 'Rating_TrungBinh', 'Rating_Cao']\n",
        "\n",
        "    # T·∫°o c·ªôt m·ªõi 'rating_category'\n",
        "    df['rating_category'] = pd.cut(df['Delivery_person_Ratings'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    print(\"‚úÖ Ph√¢n nh√≥m th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 2: X·ª¨·ª¨ L√ù ƒê·∫∂·∫∂C TR∆Ø∆ØNG TU·∫¶·∫¶N HO√ÄN (CYCLICAL FEATURES) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n...\")\n",
        "\n",
        "    # X·ª≠ l√Ω 'hour_of_day' (chu k·ª≥ 24 gi·ªù)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24.0)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24.0)\n",
        "\n",
        "    # X·ª≠ l√Ω 'day_of_week' (chu k·ª≥ 7 ng√†y)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7.0)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7.0)\n",
        "\n",
        "    print(\"‚úÖ X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 3: T·∫†·∫†O ƒê·∫∂·∫∂C TR∆Ø∆ØNG T∆Ø∆Ø∆†NG T√ÅC (INTERACTION FEATURES) ---\n",
        "    print(\"\\n‚è≥ B·∫Øt ƒë·∫ßu t·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c...\")\n",
        "\n",
        "    # T·∫°o m·ªôt h√†m ƒë·ªÉ x√°c ƒë·ªãnh gi·ªù cao ƒëi·ªÉm\n",
        "    def is_peak_hour(hour):\n",
        "        # Gi·ªù ƒÉn tr∆∞a (11h-13h) v√† gi·ªù ƒÉn t·ªëi (18h-21h)\n",
        "        if (11 <= hour <= 13) or (18 <= hour <= 21):\n",
        "            return '_PeakHour'\n",
        "        else:\n",
        "            return '_OffPeakHour'\n",
        "\n",
        "    # T·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c gi·ªØa giao th√¥ng v√† gi·ªù cao ƒëi·ªÉm\n",
        "    df['traffic_time_interaction'] = df['Road_traffic_density'].str.strip() + df['hour_of_day'].apply(is_peak_hour)\n",
        "\n",
        "    print(\"‚úÖ T·∫°o ƒë·∫∑c tr∆∞ng t∆∞∆°ng t√°c th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 4: D·ªå·ªåN D·∫∏·∫∏P V√Ä HO√ÄN THI·ªÜ·ªÜN ---\n",
        "    print(\"\\n‚è≥ D·ªçn d·∫πp c√°c c·ªôt g·ªëc...\")\n",
        "\n",
        "    # Lo·∫°i b·ªè c√°c c·ªôt g·ªëc ƒë√£ ƒë∆∞·ª£c bi·∫øn ƒë·ªïi\n",
        "    columns_to_drop = [\n",
        "        'Delivery_person_Ratings', # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'rating_category'\n",
        "        'hour_of_day',             # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'hour_sin' v√† 'hour_cos'\n",
        "        'day_of_week',             # ƒê√£ ƒë∆∞·ª£c thay b·∫±ng 'day_sin' v√† 'day_cos'\n",
        "        # Gi·ªØ l·∫°i 'Road_traffic_density' v√¨ n√≥ v·∫´n c√≥ gi√° tr·ªã\n",
        "    ]\n",
        "    df_advanced = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"‚úÖ D·ªçn d·∫πp th√†nh c√¥ng!\")\n",
        "\n",
        "\n",
        "    # --- B∆Ø·ªö·ªöC 5: XU·∫§·∫§T FILE V√Ä XEM K·∫æ·∫æT QU·∫¢·∫¢ ---\n",
        "    output_filename = 'advanced_features_dataset.csv'\n",
        "    df_advanced.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"\\nüéâ TUY·ªÜT V·ªúI! ƒê√£ x·ª≠ l√Ω v√† xu·∫•t th√†nh c√¥ng file '{output_filename}'\")\n",
        "    print(\"--- Xem 5 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£ v·ªõi c√°c c·ªôt m·ªõi: ---\")\n",
        "    print(df_advanced.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file 'processed_dataset.csv'! Vui l√≤ng t·∫£i file l√™n Colab tr∆∞·ªõc.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
